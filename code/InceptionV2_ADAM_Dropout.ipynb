{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InceptionV2_ADAM_Dropout.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe3UvwFPYKMT"
      },
      "source": [
        "## Load Data Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su96RREPI5li",
        "outputId": "2809842b-a57a-42f7-b22a-b8077ff36134",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixBthpNWrKzY"
      },
      "source": [
        "from keras.datasets import cifar100\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from keras import backend as K"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d14LEXbNqu68"
      },
      "source": [
        "(x_train, y_train_), (x_test, y_test_) = cifar100.load_data()\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4lTzEl8djyJ"
      },
      "source": [
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h_PwVSJrWjf"
      },
      "source": [
        "y_train = to_categorical(y_train_)\n",
        "y_test = to_categorical(y_test_)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsSOLpMSYEKL"
      },
      "source": [
        "## Inception V2 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlX5MwqCoSEF"
      },
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Reshape\n",
        "from keras.layers import Conv2D, Input\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers.merge import concatenate\n",
        "import os"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfDkIclipUCR"
      },
      "source": [
        "input_shape = (32, 32, 3)\n",
        "input = Input(shape=input_shape)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1Cv1ygQ7Cwj"
      },
      "source": [
        "layer1_1 = Conv2D(64, (1, 1), activation='elu', padding='same')(input)\n",
        "layer1_2 = Conv2D(96, (1, 1), activation='elu', padding='same')(input)\n",
        "layer1_2 = Conv2D(128, (3, 3), activation='elu', padding='same')(layer1_2)\n",
        "layer1_3 = Conv2D(16, (1, 1), activation='elu', padding='same')(input)\n",
        "layer1_3 = Conv2D(32, (5, 5), activation='elu', padding='same')(layer1_3)\n",
        "layer1_4 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same', data_format='channels_last')(input)\n",
        "layer1_4 = Conv2D(32, (1, 1), activation='elu', padding='same')(layer1_4)\n",
        " \n",
        "concat = concatenate([layer1_1, layer1_2, layer1_3, layer1_4])\n",
        "concat = Dropout(0.3)(concat)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgqX4ij5wn7J"
      },
      "source": [
        "layer2_1 = Conv2D(64, (1, 1), activation='elu', padding='same')(concat)\n",
        "layer2_2 = Conv2D(96, (1, 1), activation='elu', padding='same')(concat)\n",
        "layer2_2 = Conv2D(128, (3, 3), activation='elu', padding='same')(layer2_2)\n",
        "layer2_3 = Conv2D(16, (1, 1), activation='elu', padding='same')(concat)\n",
        "layer2_3 = Conv2D(32, (5, 5), activation='elu', padding='same')(layer2_3)\n",
        "layer2_4 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same', data_format='channels_last')(concat)\n",
        "layer2_4 = Conv2D(32, (1, 1), activation='elu', padding='same')(layer2_4)\n",
        "\n",
        "concat2 = concatenate([layer2_1, layer2_2, layer2_3, layer2_4])\n",
        "concat2 = Dropout(0.3)(concat2)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RzN660c7naC"
      },
      "source": [
        "# layer3_1 = Conv2D(64, (1, 1), activation='elu', padding='same')(concat2)\n",
        "# layer3_2 = Conv2D(96, (1, 1), activation='elu', padding='same')(concat2)\n",
        "# layer3_2 = Conv2D(128, (3, 3), activation='elu', padding='same')(layer3_2)\n",
        "# layer3_3 = Conv2D(16, (1, 1), activation='elu', padding='same')(concat2)\n",
        "# layer3_3 = Conv2D(32, (5, 5), activation='elu', padding='same')(layer3_3)\n",
        "# layer3_4 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same', data_format='channels_last')(concat2)\n",
        "# layer3_4 = Conv2D(32, (1, 1), activation='elu', padding='same')(layer3_4)\n",
        " \n",
        "# concat3 = concatenate([layer3_1, layer3_2, layer3_3, layer3_4])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqQohLdvpaGV",
        "outputId": "27918dd3-c8a0-420c-f7d4-d02669c5628a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# layer4_1 = Conv2D(64, (1, 1), activation='elu', padding='same')(concat3)\n",
        "# layer4_2 = Conv2D(96, (1, 1), activation='elu', padding='same')(concat3)\n",
        "# layer4_2 = Conv2D(128, (3, 3), activation='elu', padding='same')(layer4_2)\n",
        "# layer4_3 = Conv2D(16, (1, 1), activation='elu', padding='same')(concat3)\n",
        "# layer4_3 = Conv2D(32, (5, 5), activation='elu', padding='same')(layer4_3)\n",
        "# layer4_4 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same', data_format='channels_last')(concat3)\n",
        "# layer4_4 = Conv2D(32, (1, 1), activation='elu', padding='same')(layer4_4)\n",
        " \n",
        "# concat4 = concatenate([layer4_1, layer4_2, layer4_3, layer4_4])\n",
        "\n",
        "output = Conv2D(8, (3, 3), activation='elu', padding='same')(concat2)\n",
        "output = MaxPooling2D(pool_size=(3, 3))(output)\n",
        "output = Flatten()(output)                 \n",
        "output = Dense(100, activation='softmax')(output) \n",
        "\n",
        "model = Model(inputs=input, outputs=output) \n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 96)   384         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 32, 32, 16)   64          input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 3)    0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 64)   256         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 128)  110720      conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 32, 32, 32)   12832       conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 32, 32, 32)   128         max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 256)  0           conv2d_12[0][0]                  \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 32, 32, 256)  0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 32, 32, 96)   24672       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 32, 32, 16)   4112        dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 256)  0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 32, 32, 64)   16448       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 32, 32, 128)  110720      conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 32, 32, 32)   12832       conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 32, 32, 32)   8224        max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 256)  0           conv2d_18[0][0]                  \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "                                                                 conv2d_22[0][0]                  \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 32, 32, 256)  0           concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 32, 32, 8)    18440       dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 10, 10, 8)    0           conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 800)          0           max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 100)          80100       flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 399,932\n",
            "Trainable params: 399,932\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNXVRhvpsCwi"
      },
      "source": [
        "opt = keras.optimizers.Adam(learning_rate=0.0001, clipnorm=5)\n",
        "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=opt, metrics=[\"accuracy\"])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6Hea7ZxrbKJ",
        "outputId": "24efbc9d-613b-4377-959f-1ef2a157f466",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "callback = EarlyStopping(monitor='loss', patience=20)\n",
        "history = model.fit(x_train, y_train, batch_size=128, epochs=200, verbose=1, validation_data=(x_test, y_test), callbacks=[callback])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 4.3062 - accuracy: 0.0492 - val_loss: 4.0472 - val_accuracy: 0.0851\n",
            "Epoch 2/200\n",
            "391/391 [==============================] - 47s 121ms/step - loss: 3.8296 - accuracy: 0.1246 - val_loss: 3.8281 - val_accuracy: 0.1242\n",
            "Epoch 3/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 3.6178 - accuracy: 0.1633 - val_loss: 3.7014 - val_accuracy: 0.1508\n",
            "Epoch 4/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 3.4788 - accuracy: 0.1886 - val_loss: 3.5801 - val_accuracy: 0.1716\n",
            "Epoch 5/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 3.3771 - accuracy: 0.2053 - val_loss: 3.5506 - val_accuracy: 0.1787\n",
            "Epoch 6/200\n",
            "391/391 [==============================] - 47s 121ms/step - loss: 3.2788 - accuracy: 0.2245 - val_loss: 3.5868 - val_accuracy: 0.1761\n",
            "Epoch 7/200\n",
            "391/391 [==============================] - 47s 121ms/step - loss: 3.2031 - accuracy: 0.2385 - val_loss: 3.6088 - val_accuracy: 0.1748\n",
            "Epoch 8/200\n",
            "391/391 [==============================] - 47s 121ms/step - loss: 3.1367 - accuracy: 0.2499 - val_loss: 3.3695 - val_accuracy: 0.2097\n",
            "Epoch 9/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 3.0799 - accuracy: 0.2616 - val_loss: 3.5128 - val_accuracy: 0.1913\n",
            "Epoch 10/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 3.0263 - accuracy: 0.2716 - val_loss: 3.5439 - val_accuracy: 0.1918\n",
            "Epoch 11/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.9796 - accuracy: 0.2820 - val_loss: 3.5323 - val_accuracy: 0.1941\n",
            "Epoch 12/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.9356 - accuracy: 0.2878 - val_loss: 3.5031 - val_accuracy: 0.2025\n",
            "Epoch 13/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.8928 - accuracy: 0.2978 - val_loss: 3.5780 - val_accuracy: 0.1968\n",
            "Epoch 14/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.8572 - accuracy: 0.3041 - val_loss: 3.3428 - val_accuracy: 0.2238\n",
            "Epoch 15/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.8108 - accuracy: 0.3138 - val_loss: 3.5046 - val_accuracy: 0.2076\n",
            "Epoch 16/200\n",
            "391/391 [==============================] - 47s 121ms/step - loss: 2.7791 - accuracy: 0.3190 - val_loss: 3.2980 - val_accuracy: 0.2337\n",
            "Epoch 17/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.7404 - accuracy: 0.3295 - val_loss: 3.1887 - val_accuracy: 0.2459\n",
            "Epoch 18/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.7059 - accuracy: 0.3353 - val_loss: 3.1846 - val_accuracy: 0.2495\n",
            "Epoch 19/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.6733 - accuracy: 0.3448 - val_loss: 3.1395 - val_accuracy: 0.2544\n",
            "Epoch 20/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.6424 - accuracy: 0.3479 - val_loss: 3.1007 - val_accuracy: 0.2593\n",
            "Epoch 21/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.6106 - accuracy: 0.3562 - val_loss: 3.1580 - val_accuracy: 0.2560\n",
            "Epoch 22/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.5836 - accuracy: 0.3622 - val_loss: 3.1305 - val_accuracy: 0.2581\n",
            "Epoch 23/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.5598 - accuracy: 0.3675 - val_loss: 3.0658 - val_accuracy: 0.2712\n",
            "Epoch 24/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.5342 - accuracy: 0.3730 - val_loss: 3.1625 - val_accuracy: 0.2574\n",
            "Epoch 25/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.5084 - accuracy: 0.3790 - val_loss: 3.1035 - val_accuracy: 0.2640\n",
            "Epoch 26/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.4869 - accuracy: 0.3817 - val_loss: 3.0937 - val_accuracy: 0.2624\n",
            "Epoch 27/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.4662 - accuracy: 0.3867 - val_loss: 3.0477 - val_accuracy: 0.2773\n",
            "Epoch 28/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.4479 - accuracy: 0.3905 - val_loss: 3.1226 - val_accuracy: 0.2622\n",
            "Epoch 29/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.4279 - accuracy: 0.3943 - val_loss: 2.9899 - val_accuracy: 0.2844\n",
            "Epoch 30/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.4104 - accuracy: 0.3985 - val_loss: 3.0570 - val_accuracy: 0.2722\n",
            "Epoch 31/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.3916 - accuracy: 0.4044 - val_loss: 3.0106 - val_accuracy: 0.2812\n",
            "Epoch 32/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.3743 - accuracy: 0.4075 - val_loss: 3.0813 - val_accuracy: 0.2747\n",
            "Epoch 33/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.3559 - accuracy: 0.4108 - val_loss: 3.1213 - val_accuracy: 0.2671\n",
            "Epoch 34/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.3442 - accuracy: 0.4143 - val_loss: 3.0716 - val_accuracy: 0.2751\n",
            "Epoch 35/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.3273 - accuracy: 0.4169 - val_loss: 3.0068 - val_accuracy: 0.2905\n",
            "Epoch 36/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.3160 - accuracy: 0.4209 - val_loss: 3.0671 - val_accuracy: 0.2759\n",
            "Epoch 37/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.2995 - accuracy: 0.4251 - val_loss: 3.0228 - val_accuracy: 0.2873\n",
            "Epoch 38/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.2870 - accuracy: 0.4270 - val_loss: 3.0650 - val_accuracy: 0.2786\n",
            "Epoch 39/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.2684 - accuracy: 0.4313 - val_loss: 3.0659 - val_accuracy: 0.2824\n",
            "Epoch 40/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.2583 - accuracy: 0.4327 - val_loss: 3.0679 - val_accuracy: 0.2759\n",
            "Epoch 41/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.2445 - accuracy: 0.4347 - val_loss: 3.1185 - val_accuracy: 0.2690\n",
            "Epoch 42/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.2312 - accuracy: 0.4391 - val_loss: 3.1081 - val_accuracy: 0.2664\n",
            "Epoch 43/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.2203 - accuracy: 0.4404 - val_loss: 3.0287 - val_accuracy: 0.2882\n",
            "Epoch 44/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.2077 - accuracy: 0.4441 - val_loss: 3.0634 - val_accuracy: 0.2800\n",
            "Epoch 45/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.1966 - accuracy: 0.4440 - val_loss: 3.0371 - val_accuracy: 0.2812\n",
            "Epoch 46/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.1847 - accuracy: 0.4493 - val_loss: 3.0754 - val_accuracy: 0.2782\n",
            "Epoch 47/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.1735 - accuracy: 0.4504 - val_loss: 3.0283 - val_accuracy: 0.2857\n",
            "Epoch 48/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.1660 - accuracy: 0.4522 - val_loss: 3.0371 - val_accuracy: 0.2795\n",
            "Epoch 49/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.1526 - accuracy: 0.4537 - val_loss: 3.1064 - val_accuracy: 0.2735\n",
            "Epoch 50/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.1411 - accuracy: 0.4586 - val_loss: 3.0769 - val_accuracy: 0.2775\n",
            "Epoch 51/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.1337 - accuracy: 0.4612 - val_loss: 3.0889 - val_accuracy: 0.2784\n",
            "Epoch 52/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.1188 - accuracy: 0.4626 - val_loss: 3.0649 - val_accuracy: 0.2825\n",
            "Epoch 53/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.1074 - accuracy: 0.4648 - val_loss: 3.0117 - val_accuracy: 0.2910\n",
            "Epoch 54/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.0998 - accuracy: 0.4668 - val_loss: 3.0389 - val_accuracy: 0.2828\n",
            "Epoch 55/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.0901 - accuracy: 0.4672 - val_loss: 2.9387 - val_accuracy: 0.3001\n",
            "Epoch 56/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.0842 - accuracy: 0.4693 - val_loss: 3.0883 - val_accuracy: 0.2769\n",
            "Epoch 57/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.0749 - accuracy: 0.4713 - val_loss: 3.1066 - val_accuracy: 0.2747\n",
            "Epoch 58/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.0641 - accuracy: 0.4741 - val_loss: 3.0646 - val_accuracy: 0.2803\n",
            "Epoch 59/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.0540 - accuracy: 0.4752 - val_loss: 3.0947 - val_accuracy: 0.2786\n",
            "Epoch 60/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.0461 - accuracy: 0.4777 - val_loss: 3.0717 - val_accuracy: 0.2823\n",
            "Epoch 61/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.0349 - accuracy: 0.4825 - val_loss: 3.0480 - val_accuracy: 0.2817\n",
            "Epoch 62/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.0275 - accuracy: 0.4834 - val_loss: 3.0242 - val_accuracy: 0.2884\n",
            "Epoch 63/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.0166 - accuracy: 0.4835 - val_loss: 2.9981 - val_accuracy: 0.2937\n",
            "Epoch 64/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 2.0079 - accuracy: 0.4854 - val_loss: 3.0327 - val_accuracy: 0.2869\n",
            "Epoch 65/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.9998 - accuracy: 0.4877 - val_loss: 3.0262 - val_accuracy: 0.2892\n",
            "Epoch 66/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.9952 - accuracy: 0.4897 - val_loss: 3.0888 - val_accuracy: 0.2805\n",
            "Epoch 67/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.9859 - accuracy: 0.4933 - val_loss: 3.0718 - val_accuracy: 0.2856\n",
            "Epoch 68/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.9770 - accuracy: 0.4932 - val_loss: 3.0267 - val_accuracy: 0.2896\n",
            "Epoch 69/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.9697 - accuracy: 0.4931 - val_loss: 3.1539 - val_accuracy: 0.2748\n",
            "Epoch 70/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.9600 - accuracy: 0.4972 - val_loss: 3.0467 - val_accuracy: 0.2872\n",
            "Epoch 71/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.9540 - accuracy: 0.4983 - val_loss: 3.0194 - val_accuracy: 0.2937\n",
            "Epoch 72/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.9450 - accuracy: 0.4967 - val_loss: 3.0615 - val_accuracy: 0.2857\n",
            "Epoch 73/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.9385 - accuracy: 0.5006 - val_loss: 3.0791 - val_accuracy: 0.2839\n",
            "Epoch 74/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.9316 - accuracy: 0.5026 - val_loss: 3.0042 - val_accuracy: 0.2969\n",
            "Epoch 75/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.9260 - accuracy: 0.5042 - val_loss: 3.0721 - val_accuracy: 0.2856\n",
            "Epoch 76/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.9173 - accuracy: 0.5057 - val_loss: 3.0200 - val_accuracy: 0.2960\n",
            "Epoch 77/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.9131 - accuracy: 0.5079 - val_loss: 3.0639 - val_accuracy: 0.2896\n",
            "Epoch 78/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.9056 - accuracy: 0.5096 - val_loss: 3.0957 - val_accuracy: 0.2843\n",
            "Epoch 79/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.8982 - accuracy: 0.5096 - val_loss: 3.0314 - val_accuracy: 0.2916\n",
            "Epoch 80/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.8895 - accuracy: 0.5125 - val_loss: 3.1285 - val_accuracy: 0.2747\n",
            "Epoch 81/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.8839 - accuracy: 0.5122 - val_loss: 3.0214 - val_accuracy: 0.2990\n",
            "Epoch 82/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.8735 - accuracy: 0.5165 - val_loss: 3.1502 - val_accuracy: 0.2805\n",
            "Epoch 83/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.8664 - accuracy: 0.5178 - val_loss: 3.1832 - val_accuracy: 0.2723\n",
            "Epoch 84/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.8639 - accuracy: 0.5181 - val_loss: 3.1481 - val_accuracy: 0.2823\n",
            "Epoch 85/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.8568 - accuracy: 0.5220 - val_loss: 3.0824 - val_accuracy: 0.2875\n",
            "Epoch 86/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.8505 - accuracy: 0.5202 - val_loss: 2.9890 - val_accuracy: 0.3037\n",
            "Epoch 87/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.8443 - accuracy: 0.5225 - val_loss: 3.0164 - val_accuracy: 0.2999\n",
            "Epoch 88/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.8389 - accuracy: 0.5213 - val_loss: 3.0493 - val_accuracy: 0.2959\n",
            "Epoch 89/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.8322 - accuracy: 0.5234 - val_loss: 3.0168 - val_accuracy: 0.2998\n",
            "Epoch 90/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.8248 - accuracy: 0.5264 - val_loss: 3.0584 - val_accuracy: 0.2969\n",
            "Epoch 91/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.8200 - accuracy: 0.5287 - val_loss: 3.1226 - val_accuracy: 0.2796\n",
            "Epoch 92/200\n",
            "391/391 [==============================] - 47s 121ms/step - loss: 1.8140 - accuracy: 0.5293 - val_loss: 3.0983 - val_accuracy: 0.2837\n",
            "Epoch 93/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.8109 - accuracy: 0.5296 - val_loss: 3.0441 - val_accuracy: 0.2952\n",
            "Epoch 94/200\n",
            "391/391 [==============================] - 47s 121ms/step - loss: 1.8008 - accuracy: 0.5309 - val_loss: 3.1591 - val_accuracy: 0.2822\n",
            "Epoch 95/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.7981 - accuracy: 0.5319 - val_loss: 3.0746 - val_accuracy: 0.2917\n",
            "Epoch 96/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.7896 - accuracy: 0.5319 - val_loss: 3.1355 - val_accuracy: 0.2860\n",
            "Epoch 97/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.7832 - accuracy: 0.5361 - val_loss: 3.0791 - val_accuracy: 0.2947\n",
            "Epoch 98/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.7766 - accuracy: 0.5391 - val_loss: 3.1074 - val_accuracy: 0.2906\n",
            "Epoch 99/200\n",
            "391/391 [==============================] - 47s 121ms/step - loss: 1.7728 - accuracy: 0.5386 - val_loss: 3.1448 - val_accuracy: 0.2819\n",
            "Epoch 100/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.7665 - accuracy: 0.5395 - val_loss: 3.1208 - val_accuracy: 0.2890\n",
            "Epoch 101/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.7649 - accuracy: 0.5373 - val_loss: 3.1016 - val_accuracy: 0.2935\n",
            "Epoch 102/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.7572 - accuracy: 0.5427 - val_loss: 3.0810 - val_accuracy: 0.2919\n",
            "Epoch 103/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.7528 - accuracy: 0.5410 - val_loss: 3.1755 - val_accuracy: 0.2851\n",
            "Epoch 104/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.7451 - accuracy: 0.5441 - val_loss: 3.1214 - val_accuracy: 0.2916\n",
            "Epoch 105/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.7431 - accuracy: 0.5444 - val_loss: 3.1640 - val_accuracy: 0.2843\n",
            "Epoch 106/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.7362 - accuracy: 0.5461 - val_loss: 3.0760 - val_accuracy: 0.3003\n",
            "Epoch 107/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.7347 - accuracy: 0.5443 - val_loss: 3.0845 - val_accuracy: 0.2944\n",
            "Epoch 108/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.7254 - accuracy: 0.5488 - val_loss: 3.0797 - val_accuracy: 0.2970\n",
            "Epoch 109/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.7223 - accuracy: 0.5465 - val_loss: 3.1375 - val_accuracy: 0.2913\n",
            "Epoch 110/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.7228 - accuracy: 0.5493 - val_loss: 3.1254 - val_accuracy: 0.2943\n",
            "Epoch 111/200\n",
            "391/391 [==============================] - 47s 121ms/step - loss: 1.7124 - accuracy: 0.5498 - val_loss: 3.0426 - val_accuracy: 0.3015\n",
            "Epoch 112/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.7089 - accuracy: 0.5520 - val_loss: 3.0824 - val_accuracy: 0.2991\n",
            "Epoch 113/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.7082 - accuracy: 0.5528 - val_loss: 3.0813 - val_accuracy: 0.2974\n",
            "Epoch 114/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.6984 - accuracy: 0.5525 - val_loss: 3.0547 - val_accuracy: 0.3003\n",
            "Epoch 115/200\n",
            "391/391 [==============================] - 47s 121ms/step - loss: 1.6981 - accuracy: 0.5511 - val_loss: 3.0452 - val_accuracy: 0.3026\n",
            "Epoch 116/200\n",
            "391/391 [==============================] - 47s 121ms/step - loss: 1.6921 - accuracy: 0.5539 - val_loss: 3.1373 - val_accuracy: 0.2922\n",
            "Epoch 117/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.6856 - accuracy: 0.5565 - val_loss: 3.1346 - val_accuracy: 0.2946\n",
            "Epoch 118/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.6846 - accuracy: 0.5568 - val_loss: 3.1480 - val_accuracy: 0.2896\n",
            "Epoch 119/200\n",
            "391/391 [==============================] - 47s 121ms/step - loss: 1.6775 - accuracy: 0.5577 - val_loss: 3.1448 - val_accuracy: 0.2969\n",
            "Epoch 120/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.6732 - accuracy: 0.5616 - val_loss: 3.0926 - val_accuracy: 0.2999\n",
            "Epoch 121/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.6698 - accuracy: 0.5576 - val_loss: 3.0846 - val_accuracy: 0.3000\n",
            "Epoch 122/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.6604 - accuracy: 0.5610 - val_loss: 3.1152 - val_accuracy: 0.2972\n",
            "Epoch 123/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.6615 - accuracy: 0.5607 - val_loss: 3.1699 - val_accuracy: 0.2929\n",
            "Epoch 124/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.6571 - accuracy: 0.5624 - val_loss: 3.1503 - val_accuracy: 0.2996\n",
            "Epoch 125/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.6568 - accuracy: 0.5613 - val_loss: 3.1058 - val_accuracy: 0.3001\n",
            "Epoch 126/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.6499 - accuracy: 0.5643 - val_loss: 3.1353 - val_accuracy: 0.2962\n",
            "Epoch 127/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.6443 - accuracy: 0.5630 - val_loss: 3.1484 - val_accuracy: 0.2956\n",
            "Epoch 128/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.6398 - accuracy: 0.5670 - val_loss: 3.1911 - val_accuracy: 0.2945\n",
            "Epoch 129/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.6390 - accuracy: 0.5660 - val_loss: 3.1303 - val_accuracy: 0.2983\n",
            "Epoch 130/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.6310 - accuracy: 0.5679 - val_loss: 3.1497 - val_accuracy: 0.2941\n",
            "Epoch 131/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.6271 - accuracy: 0.5679 - val_loss: 3.1212 - val_accuracy: 0.3011\n",
            "Epoch 132/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.6235 - accuracy: 0.5689 - val_loss: 3.1456 - val_accuracy: 0.2974\n",
            "Epoch 133/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.6171 - accuracy: 0.5713 - val_loss: 3.2083 - val_accuracy: 0.2955\n",
            "Epoch 134/200\n",
            "391/391 [==============================] - 47s 119ms/step - loss: 1.6150 - accuracy: 0.5711 - val_loss: 3.1110 - val_accuracy: 0.3009\n",
            "Epoch 135/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.6094 - accuracy: 0.5723 - val_loss: 3.1021 - val_accuracy: 0.3034\n",
            "Epoch 136/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.6086 - accuracy: 0.5711 - val_loss: 3.0967 - val_accuracy: 0.3038\n",
            "Epoch 137/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.6060 - accuracy: 0.5721 - val_loss: 3.1606 - val_accuracy: 0.3001\n",
            "Epoch 138/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5999 - accuracy: 0.5722 - val_loss: 3.1366 - val_accuracy: 0.3009\n",
            "Epoch 139/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5946 - accuracy: 0.5758 - val_loss: 3.1468 - val_accuracy: 0.3001\n",
            "Epoch 140/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5962 - accuracy: 0.5764 - val_loss: 3.0670 - val_accuracy: 0.3112\n",
            "Epoch 141/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5931 - accuracy: 0.5756 - val_loss: 3.0743 - val_accuracy: 0.3074\n",
            "Epoch 142/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5839 - accuracy: 0.5764 - val_loss: 3.1298 - val_accuracy: 0.3017\n",
            "Epoch 143/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5811 - accuracy: 0.5796 - val_loss: 3.1455 - val_accuracy: 0.3056\n",
            "Epoch 144/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5791 - accuracy: 0.5793 - val_loss: 3.0946 - val_accuracy: 0.3073\n",
            "Epoch 145/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5765 - accuracy: 0.5791 - val_loss: 3.1236 - val_accuracy: 0.3047\n",
            "Epoch 146/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5722 - accuracy: 0.5813 - val_loss: 3.1186 - val_accuracy: 0.3039\n",
            "Epoch 147/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5678 - accuracy: 0.5816 - val_loss: 3.1343 - val_accuracy: 0.3061\n",
            "Epoch 148/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5645 - accuracy: 0.5818 - val_loss: 3.1114 - val_accuracy: 0.3072\n",
            "Epoch 149/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5643 - accuracy: 0.5824 - val_loss: 3.1132 - val_accuracy: 0.3045\n",
            "Epoch 150/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5588 - accuracy: 0.5826 - val_loss: 3.1337 - val_accuracy: 0.3078\n",
            "Epoch 151/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5564 - accuracy: 0.5821 - val_loss: 3.1010 - val_accuracy: 0.3100\n",
            "Epoch 152/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5541 - accuracy: 0.5845 - val_loss: 3.1647 - val_accuracy: 0.3001\n",
            "Epoch 153/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5454 - accuracy: 0.5861 - val_loss: 3.1612 - val_accuracy: 0.3084\n",
            "Epoch 154/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5481 - accuracy: 0.5849 - val_loss: 3.1859 - val_accuracy: 0.2980\n",
            "Epoch 155/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5393 - accuracy: 0.5864 - val_loss: 3.2094 - val_accuracy: 0.2964\n",
            "Epoch 156/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5424 - accuracy: 0.5836 - val_loss: 3.1502 - val_accuracy: 0.3049\n",
            "Epoch 157/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5345 - accuracy: 0.5901 - val_loss: 3.1511 - val_accuracy: 0.3075\n",
            "Epoch 158/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5341 - accuracy: 0.5897 - val_loss: 3.1340 - val_accuracy: 0.3073\n",
            "Epoch 159/200\n",
            "391/391 [==============================] - 47s 119ms/step - loss: 1.5320 - accuracy: 0.5875 - val_loss: 3.1247 - val_accuracy: 0.3091\n",
            "Epoch 160/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5245 - accuracy: 0.5878 - val_loss: 3.1700 - val_accuracy: 0.3049\n",
            "Epoch 161/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5249 - accuracy: 0.5894 - val_loss: 3.1594 - val_accuracy: 0.3087\n",
            "Epoch 162/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5187 - accuracy: 0.5894 - val_loss: 3.1821 - val_accuracy: 0.3032\n",
            "Epoch 163/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5181 - accuracy: 0.5922 - val_loss: 3.2122 - val_accuracy: 0.2991\n",
            "Epoch 164/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5109 - accuracy: 0.5930 - val_loss: 3.1839 - val_accuracy: 0.3052\n",
            "Epoch 165/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5123 - accuracy: 0.5947 - val_loss: 3.1270 - val_accuracy: 0.3133\n",
            "Epoch 166/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5102 - accuracy: 0.5957 - val_loss: 3.1773 - val_accuracy: 0.3095\n",
            "Epoch 167/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5045 - accuracy: 0.5954 - val_loss: 3.1535 - val_accuracy: 0.3098\n",
            "Epoch 168/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5025 - accuracy: 0.5949 - val_loss: 3.1207 - val_accuracy: 0.3116\n",
            "Epoch 169/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.5032 - accuracy: 0.5929 - val_loss: 3.1729 - val_accuracy: 0.3093\n",
            "Epoch 170/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4972 - accuracy: 0.5969 - val_loss: 3.2322 - val_accuracy: 0.2991\n",
            "Epoch 171/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4926 - accuracy: 0.5989 - val_loss: 3.1685 - val_accuracy: 0.3109\n",
            "Epoch 172/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4897 - accuracy: 0.5978 - val_loss: 3.1769 - val_accuracy: 0.3093\n",
            "Epoch 173/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4881 - accuracy: 0.5974 - val_loss: 3.2229 - val_accuracy: 0.3042\n",
            "Epoch 174/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4879 - accuracy: 0.5982 - val_loss: 3.1554 - val_accuracy: 0.3108\n",
            "Epoch 175/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4811 - accuracy: 0.5999 - val_loss: 3.2012 - val_accuracy: 0.3030\n",
            "Epoch 176/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4779 - accuracy: 0.6024 - val_loss: 3.2417 - val_accuracy: 0.2999\n",
            "Epoch 177/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4774 - accuracy: 0.6005 - val_loss: 3.1578 - val_accuracy: 0.3123\n",
            "Epoch 178/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4736 - accuracy: 0.6008 - val_loss: 3.2038 - val_accuracy: 0.3071\n",
            "Epoch 179/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4712 - accuracy: 0.6010 - val_loss: 3.1728 - val_accuracy: 0.3101\n",
            "Epoch 180/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4617 - accuracy: 0.6053 - val_loss: 3.1454 - val_accuracy: 0.3166\n",
            "Epoch 181/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4629 - accuracy: 0.6030 - val_loss: 3.1918 - val_accuracy: 0.3105\n",
            "Epoch 182/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4633 - accuracy: 0.6051 - val_loss: 3.2131 - val_accuracy: 0.3075\n",
            "Epoch 183/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4642 - accuracy: 0.6030 - val_loss: 3.1294 - val_accuracy: 0.3152\n",
            "Epoch 184/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4532 - accuracy: 0.6060 - val_loss: 3.2253 - val_accuracy: 0.3055\n",
            "Epoch 185/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4560 - accuracy: 0.6055 - val_loss: 3.1503 - val_accuracy: 0.3176\n",
            "Epoch 186/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4500 - accuracy: 0.6073 - val_loss: 3.2021 - val_accuracy: 0.3074\n",
            "Epoch 187/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4466 - accuracy: 0.6059 - val_loss: 3.2161 - val_accuracy: 0.3096\n",
            "Epoch 188/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4409 - accuracy: 0.6091 - val_loss: 3.2451 - val_accuracy: 0.3059\n",
            "Epoch 189/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4446 - accuracy: 0.6059 - val_loss: 3.2052 - val_accuracy: 0.3129\n",
            "Epoch 190/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4458 - accuracy: 0.6052 - val_loss: 3.2635 - val_accuracy: 0.2987\n",
            "Epoch 191/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4421 - accuracy: 0.6071 - val_loss: 3.2140 - val_accuracy: 0.3121\n",
            "Epoch 192/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4338 - accuracy: 0.6096 - val_loss: 3.2239 - val_accuracy: 0.3101\n",
            "Epoch 193/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4314 - accuracy: 0.6107 - val_loss: 3.2243 - val_accuracy: 0.3092\n",
            "Epoch 194/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4316 - accuracy: 0.6112 - val_loss: 3.2439 - val_accuracy: 0.3071\n",
            "Epoch 195/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4237 - accuracy: 0.6120 - val_loss: 3.2761 - val_accuracy: 0.3041\n",
            "Epoch 196/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4209 - accuracy: 0.6148 - val_loss: 3.2330 - val_accuracy: 0.3119\n",
            "Epoch 197/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4200 - accuracy: 0.6148 - val_loss: 3.2440 - val_accuracy: 0.3075\n",
            "Epoch 198/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4229 - accuracy: 0.6131 - val_loss: 3.2519 - val_accuracy: 0.3099\n",
            "Epoch 199/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4228 - accuracy: 0.6122 - val_loss: 3.2615 - val_accuracy: 0.3080\n",
            "Epoch 200/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 1.4169 - accuracy: 0.6141 - val_loss: 3.2787 - val_accuracy: 0.3058\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_QICA0vECJf"
      },
      "source": [
        "model.save_weights(filepath=F\"/content/gdrive/My Drive/Checkpoints/InceptionV2_ADAM_Dropout.h5\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK9Ww8AJYaaM"
      },
      "source": [
        "## Test Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZuWXGeW-nqY",
        "outputId": "05772528-4541-428b-d1ee-2ce51f300c8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "print(history_dict.keys())\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(history_dict['accuracy']) + 1)\n",
        "\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfO0lHJIE0Gy",
        "outputId": "7d8ee815-1ffe-44c9-9c25-288d4902adca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "y_pred = model.predict(x_test).argmax(-1)\n",
        "import sklearn.metrics as metrics\n",
        "print(metrics.classification_report(y_test.argmax(axis=1), y_pred))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.51      0.54       100\n",
            "           1       0.28      0.45      0.34       100\n",
            "           2       0.24      0.07      0.11       100\n",
            "           3       0.16      0.21      0.18       100\n",
            "           4       0.10      0.28      0.15       100\n",
            "           5       0.31      0.18      0.23       100\n",
            "           6       0.33      0.33      0.33       100\n",
            "           7       0.31      0.37      0.34       100\n",
            "           8       0.59      0.38      0.46       100\n",
            "           9       0.58      0.45      0.51       100\n",
            "          10       0.23      0.21      0.22       100\n",
            "          11       0.25      0.11      0.15       100\n",
            "          12       0.30      0.35      0.33       100\n",
            "          13       0.35      0.26      0.30       100\n",
            "          14       0.27      0.28      0.27       100\n",
            "          15       0.26      0.11      0.15       100\n",
            "          16       0.46      0.27      0.34       100\n",
            "          17       0.48      0.32      0.39       100\n",
            "          18       0.24      0.25      0.24       100\n",
            "          19       0.24      0.12      0.16       100\n",
            "          20       0.78      0.47      0.59       100\n",
            "          21       0.38      0.62      0.47       100\n",
            "          22       0.45      0.29      0.35       100\n",
            "          23       0.30      0.59      0.40       100\n",
            "          24       0.42      0.60      0.49       100\n",
            "          25       0.25      0.20      0.22       100\n",
            "          26       0.26      0.26      0.26       100\n",
            "          27       0.18      0.37      0.24       100\n",
            "          28       0.58      0.42      0.49       100\n",
            "          29       0.36      0.18      0.24       100\n",
            "          30       0.32      0.38      0.35       100\n",
            "          31       0.33      0.35      0.34       100\n",
            "          32       0.27      0.21      0.24       100\n",
            "          33       0.16      0.45      0.24       100\n",
            "          34       0.27      0.23      0.25       100\n",
            "          35       0.23      0.10      0.14       100\n",
            "          36       0.42      0.27      0.33       100\n",
            "          37       0.56      0.20      0.29       100\n",
            "          38       0.16      0.10      0.12       100\n",
            "          39       0.39      0.16      0.23       100\n",
            "          40       0.42      0.28      0.34       100\n",
            "          41       0.64      0.58      0.61       100\n",
            "          42       0.23      0.36      0.28       100\n",
            "          43       0.28      0.27      0.28       100\n",
            "          44       0.16      0.14      0.15       100\n",
            "          45       0.17      0.21      0.19       100\n",
            "          46       0.22      0.14      0.17       100\n",
            "          47       0.28      0.43      0.34       100\n",
            "          48       0.63      0.64      0.63       100\n",
            "          49       0.39      0.30      0.34       100\n",
            "          50       0.11      0.07      0.09       100\n",
            "          51       0.30      0.16      0.21       100\n",
            "          52       0.38      0.54      0.45       100\n",
            "          53       0.54      0.42      0.47       100\n",
            "          54       0.36      0.23      0.28       100\n",
            "          55       0.14      0.12      0.13       100\n",
            "          56       0.41      0.44      0.43       100\n",
            "          57       0.43      0.20      0.27       100\n",
            "          58       0.49      0.34      0.40       100\n",
            "          59       0.19      0.34      0.24       100\n",
            "          60       0.57      0.64      0.60       100\n",
            "          61       0.55      0.43      0.48       100\n",
            "          62       0.25      0.30      0.27       100\n",
            "          63       0.18      0.48      0.26       100\n",
            "          64       0.05      0.05      0.05       100\n",
            "          65       0.14      0.08      0.10       100\n",
            "          66       0.32      0.17      0.22       100\n",
            "          67       0.25      0.31      0.28       100\n",
            "          68       0.50      0.69      0.58       100\n",
            "          69       0.49      0.43      0.46       100\n",
            "          70       0.36      0.30      0.33       100\n",
            "          71       0.36      0.59      0.45       100\n",
            "          72       0.11      0.10      0.10       100\n",
            "          73       0.30      0.17      0.22       100\n",
            "          74       0.15      0.26      0.19       100\n",
            "          75       0.53      0.51      0.52       100\n",
            "          76       0.53      0.54      0.53       100\n",
            "          77       0.15      0.18      0.17       100\n",
            "          78       0.13      0.09      0.11       100\n",
            "          79       0.25      0.31      0.28       100\n",
            "          80       0.08      0.07      0.08       100\n",
            "          81       0.31      0.23      0.26       100\n",
            "          82       0.25      0.71      0.37       100\n",
            "          83       0.33      0.32      0.33       100\n",
            "          84       0.36      0.14      0.20       100\n",
            "          85       0.35      0.50      0.41       100\n",
            "          86       0.52      0.34      0.41       100\n",
            "          87       0.57      0.37      0.45       100\n",
            "          88       0.27      0.17      0.21       100\n",
            "          89       0.31      0.32      0.32       100\n",
            "          90       0.26      0.25      0.25       100\n",
            "          91       0.44      0.39      0.41       100\n",
            "          92       0.23      0.12      0.16       100\n",
            "          93       0.13      0.19      0.15       100\n",
            "          94       0.62      0.50      0.56       100\n",
            "          95       0.39      0.38      0.39       100\n",
            "          96       0.18      0.42      0.25       100\n",
            "          97       0.36      0.18      0.24       100\n",
            "          98       0.17      0.05      0.08       100\n",
            "          99       0.20      0.13      0.16       100\n",
            "\n",
            "    accuracy                           0.31     10000\n",
            "   macro avg       0.33      0.31      0.30     10000\n",
            "weighted avg       0.33      0.31      0.30     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8SADQsAT1JV"
      },
      "source": [
        "## TESTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4Smel9NqS1r"
      },
      "source": [
        "# model.load_weights(filepath=F\"/content/gdrive/My Drive/Checkpoints/InceptionV2_ADAM_Dropout.h5\")\n",
        "\n",
        "model.load_weights(\"InceptionV2_ADAM_Dropout.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0XDBMyDq0d3",
        "outputId": "b22d4457-d63a-4396-9549-16c1a018cd8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "result = model.evaluate(batch_size=128, x=x_test, y=y_test)\n",
        "dict(zip(model.metrics_names,result))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79/79 [==============================] - 3s 32ms/step - loss: 3.2787 - accuracy: 0.3058\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.3057999908924103, 'loss': 3.2787327766418457}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgZYqnfyWKOM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}