{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InceptionV2_SGD_Dropout.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe3UvwFPYKMT"
      },
      "source": [
        "## Load Data Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su96RREPI5li",
        "outputId": "ae402a29-a0d3-4509-eee9-ab567a1b2233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixBthpNWrKzY"
      },
      "source": [
        "from keras.datasets import cifar100\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from keras import backend as K"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d14LEXbNqu68",
        "outputId": "1ac23bbd-b0ae-4cfe-a976-a124704aca30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "(x_train, y_train_), (x_test, y_test_) = cifar100.load_data()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4lTzEl8djyJ"
      },
      "source": [
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h_PwVSJrWjf"
      },
      "source": [
        "y_train = to_categorical(y_train_)\n",
        "y_test = to_categorical(y_test_)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsSOLpMSYEKL"
      },
      "source": [
        "## Inception V2 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlX5MwqCoSEF"
      },
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Reshape\n",
        "from keras.layers import Conv2D, Input\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers.merge import concatenate\n",
        "import os"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfDkIclipUCR"
      },
      "source": [
        "input_shape = (32, 32, 3)\n",
        "input = Input(shape=input_shape)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1Cv1ygQ7Cwj"
      },
      "source": [
        "layer1_1 = Conv2D(64, (1, 1), activation='elu', padding='same')(input)\n",
        "layer1_2 = Conv2D(96, (1, 1), activation='elu', padding='same')(input)\n",
        "layer1_2 = Conv2D(128, (3, 3), activation='elu', padding='same')(layer1_2)\n",
        "layer1_3 = Conv2D(16, (1, 1), activation='elu', padding='same')(input)\n",
        "layer1_3 = Conv2D(32, (5, 5), activation='elu', padding='same')(layer1_3)\n",
        "layer1_4 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same', data_format='channels_last')(input)\n",
        "layer1_4 = Conv2D(32, (1, 1), activation='elu', padding='same')(layer1_4)\n",
        " \n",
        "concat = concatenate([layer1_1, layer1_2, layer1_3, layer1_4])\n",
        "concat = Dropout(0.3)(concat)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgqX4ij5wn7J"
      },
      "source": [
        "layer2_1 = Conv2D(64, (1, 1), activation='elu', padding='same')(concat)\n",
        "layer2_2 = Conv2D(96, (1, 1), activation='elu', padding='same')(concat)\n",
        "layer2_2 = Conv2D(128, (3, 3), activation='elu', padding='same')(layer2_2)\n",
        "layer2_3 = Conv2D(16, (1, 1), activation='elu', padding='same')(concat)\n",
        "layer2_3 = Conv2D(32, (5, 5), activation='elu', padding='same')(layer2_3)\n",
        "layer2_4 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same', data_format='channels_last')(concat)\n",
        "layer2_4 = Conv2D(32, (1, 1), activation='elu', padding='same')(layer2_4)\n",
        "\n",
        "concat2 = concatenate([layer2_1, layer2_2, layer2_3, layer2_4])\n",
        "concat2 = Dropout(0.3)(concat2)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RzN660c7naC"
      },
      "source": [
        "# layer3_1 = Conv2D(64, (1, 1), activation='elu', padding='same')(concat2)\n",
        "# layer3_2 = Conv2D(96, (1, 1), activation='elu', padding='same')(concat2)\n",
        "# layer3_2 = Conv2D(128, (3, 3), activation='elu', padding='same')(layer3_2)\n",
        "# layer3_3 = Conv2D(16, (1, 1), activation='elu', padding='same')(concat2)\n",
        "# layer3_3 = Conv2D(32, (5, 5), activation='elu', padding='same')(layer3_3)\n",
        "# layer3_4 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same', data_format='channels_last')(concat2)\n",
        "# layer3_4 = Conv2D(32, (1, 1), activation='elu', padding='same')(layer3_4)\n",
        " \n",
        "# concat3 = concatenate([layer3_1, layer3_2, layer3_3, layer3_4])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqQohLdvpaGV",
        "outputId": "27b1fb0a-38ae-477a-803b-9e7ef894b88b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# layer4_1 = Conv2D(64, (1, 1), activation='elu', padding='same')(concat3)\n",
        "# layer4_2 = Conv2D(96, (1, 1), activation='elu', padding='same')(concat3)\n",
        "# layer4_2 = Conv2D(128, (3, 3), activation='elu', padding='same')(layer4_2)\n",
        "# layer4_3 = Conv2D(16, (1, 1), activation='elu', padding='same')(concat3)\n",
        "# layer4_3 = Conv2D(32, (5, 5), activation='elu', padding='same')(layer4_3)\n",
        "# layer4_4 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same', data_format='channels_last')(concat3)\n",
        "# layer4_4 = Conv2D(32, (1, 1), activation='elu', padding='same')(layer4_4)\n",
        " \n",
        "# concat4 = concatenate([layer4_1, layer4_2, layer4_3, layer4_4])\n",
        "\n",
        "output = Conv2D(8, (3, 3), activation='elu', padding='same')(concat2)\n",
        "output = MaxPooling2D(pool_size=(3, 3))(output)\n",
        "output = Flatten()(output)                 \n",
        "output = Dense(100, activation='softmax')(output) \n",
        "\n",
        "model = Model(inputs=input, outputs=output) \n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 96)   384         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 16)   64          input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 32, 32, 3)    0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 64)   256         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 128)  110720      conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 32)   12832       conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 32)   128         max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 32, 32, 256)  0           conv2d[0][0]                     \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 32, 32, 256)  0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 96)   24672       dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 16)   4112        dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 256)  0           dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 64)   16448       dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 128)  110720      conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 32)   12832       conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 32)   8224        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 256)  0           conv2d_6[0][0]                   \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 32, 32, 256)  0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 8)    18440       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 10, 10, 8)    0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 800)          0           max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 100)          80100       flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 399,932\n",
            "Trainable params: 399,932\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNXVRhvpsCwi"
      },
      "source": [
        "opt = keras.optimizers.SGD(learning_rate=0.01, clipnorm=5)\n",
        "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=opt, metrics=[\"accuracy\"])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6Hea7ZxrbKJ",
        "outputId": "749780f1-6750-4fa0-873a-4376e6ea5003",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "callback = EarlyStopping(monitor='loss', patience=20)\n",
        "history = model.fit(x_train, y_train, batch_size=128, epochs=200, verbose=1, validation_data=(x_test, y_test), callbacks=[callback])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 4.5381 - accuracy: 0.0194 - val_loss: 4.4123 - val_accuracy: 0.0282\n",
            "Epoch 2/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 4.2655 - accuracy: 0.0544 - val_loss: 4.1703 - val_accuracy: 0.0675\n",
            "Epoch 3/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 4.0218 - accuracy: 0.0941 - val_loss: 3.9610 - val_accuracy: 0.1059\n",
            "Epoch 4/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 3.8556 - accuracy: 0.1215 - val_loss: 3.8383 - val_accuracy: 0.1247\n",
            "Epoch 5/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 3.7373 - accuracy: 0.1447 - val_loss: 3.7469 - val_accuracy: 0.1433\n",
            "Epoch 6/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 3.6446 - accuracy: 0.1618 - val_loss: 3.6761 - val_accuracy: 0.1569\n",
            "Epoch 7/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 3.5672 - accuracy: 0.1760 - val_loss: 3.6319 - val_accuracy: 0.1630\n",
            "Epoch 8/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 3.5002 - accuracy: 0.1873 - val_loss: 3.6096 - val_accuracy: 0.1676\n",
            "Epoch 9/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 3.4365 - accuracy: 0.1997 - val_loss: 3.5242 - val_accuracy: 0.1837\n",
            "Epoch 10/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 3.3774 - accuracy: 0.2107 - val_loss: 3.5641 - val_accuracy: 0.1781\n",
            "Epoch 11/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 3.3209 - accuracy: 0.2202 - val_loss: 3.5094 - val_accuracy: 0.1893\n",
            "Epoch 12/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 3.2682 - accuracy: 0.2293 - val_loss: 3.4881 - val_accuracy: 0.1886\n",
            "Epoch 13/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 3.2175 - accuracy: 0.2398 - val_loss: 3.4396 - val_accuracy: 0.2013\n",
            "Epoch 14/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 3.1678 - accuracy: 0.2478 - val_loss: 3.4013 - val_accuracy: 0.2063\n",
            "Epoch 15/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 3.1219 - accuracy: 0.2576 - val_loss: 3.3950 - val_accuracy: 0.2092\n",
            "Epoch 16/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 3.0801 - accuracy: 0.2638 - val_loss: 3.4550 - val_accuracy: 0.2009\n",
            "Epoch 17/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 3.0422 - accuracy: 0.2730 - val_loss: 3.4810 - val_accuracy: 0.1971\n",
            "Epoch 18/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 3.0110 - accuracy: 0.2760 - val_loss: 3.4955 - val_accuracy: 0.2003\n",
            "Epoch 19/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.9771 - accuracy: 0.2863 - val_loss: 3.4938 - val_accuracy: 0.2005\n",
            "Epoch 20/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.9468 - accuracy: 0.2902 - val_loss: 3.3424 - val_accuracy: 0.2216\n",
            "Epoch 21/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.9194 - accuracy: 0.2951 - val_loss: 3.3246 - val_accuracy: 0.2243\n",
            "Epoch 22/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.8912 - accuracy: 0.3015 - val_loss: 3.3483 - val_accuracy: 0.2246\n",
            "Epoch 23/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.8641 - accuracy: 0.3071 - val_loss: 3.2932 - val_accuracy: 0.2319\n",
            "Epoch 24/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.8397 - accuracy: 0.3108 - val_loss: 3.3859 - val_accuracy: 0.2210\n",
            "Epoch 25/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.8140 - accuracy: 0.3165 - val_loss: 3.4006 - val_accuracy: 0.2125\n",
            "Epoch 26/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.7902 - accuracy: 0.3208 - val_loss: 3.3256 - val_accuracy: 0.2288\n",
            "Epoch 27/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.7660 - accuracy: 0.3257 - val_loss: 3.4292 - val_accuracy: 0.2138\n",
            "Epoch 28/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.7409 - accuracy: 0.3324 - val_loss: 3.3244 - val_accuracy: 0.2292\n",
            "Epoch 29/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.7183 - accuracy: 0.3354 - val_loss: 3.3032 - val_accuracy: 0.2316\n",
            "Epoch 30/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.6970 - accuracy: 0.3433 - val_loss: 3.3060 - val_accuracy: 0.2286\n",
            "Epoch 31/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.6733 - accuracy: 0.3450 - val_loss: 3.3050 - val_accuracy: 0.2347\n",
            "Epoch 32/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.6491 - accuracy: 0.3505 - val_loss: 3.2249 - val_accuracy: 0.2414\n",
            "Epoch 33/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.6256 - accuracy: 0.3547 - val_loss: 3.2194 - val_accuracy: 0.2487\n",
            "Epoch 34/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.6006 - accuracy: 0.3605 - val_loss: 3.1687 - val_accuracy: 0.2557\n",
            "Epoch 35/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.5781 - accuracy: 0.3655 - val_loss: 3.2949 - val_accuracy: 0.2378\n",
            "Epoch 36/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.5562 - accuracy: 0.3685 - val_loss: 3.3166 - val_accuracy: 0.2275\n",
            "Epoch 37/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.5353 - accuracy: 0.3741 - val_loss: 3.2296 - val_accuracy: 0.2443\n",
            "Epoch 38/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.5161 - accuracy: 0.3780 - val_loss: 3.2010 - val_accuracy: 0.2487\n",
            "Epoch 39/200\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 2.4924 - accuracy: 0.3844 - val_loss: 3.1014 - val_accuracy: 0.2635\n",
            "Epoch 40/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.4757 - accuracy: 0.3870 - val_loss: 3.1576 - val_accuracy: 0.2573\n",
            "Epoch 41/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.4559 - accuracy: 0.3900 - val_loss: 3.1482 - val_accuracy: 0.2581\n",
            "Epoch 42/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.4388 - accuracy: 0.3954 - val_loss: 3.1351 - val_accuracy: 0.2608\n",
            "Epoch 43/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.4234 - accuracy: 0.3987 - val_loss: 3.0102 - val_accuracy: 0.2739\n",
            "Epoch 44/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.4056 - accuracy: 0.4020 - val_loss: 3.0121 - val_accuracy: 0.2755\n",
            "Epoch 45/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.3892 - accuracy: 0.4060 - val_loss: 3.1104 - val_accuracy: 0.2658\n",
            "Epoch 46/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.3731 - accuracy: 0.4087 - val_loss: 3.0718 - val_accuracy: 0.2707\n",
            "Epoch 47/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.3591 - accuracy: 0.4117 - val_loss: 3.0551 - val_accuracy: 0.2736\n",
            "Epoch 48/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.3428 - accuracy: 0.4156 - val_loss: 3.0440 - val_accuracy: 0.2757\n",
            "Epoch 49/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.3310 - accuracy: 0.4174 - val_loss: 3.0136 - val_accuracy: 0.2805\n",
            "Epoch 50/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.3165 - accuracy: 0.4208 - val_loss: 3.1151 - val_accuracy: 0.2636\n",
            "Epoch 51/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.3034 - accuracy: 0.4229 - val_loss: 2.9751 - val_accuracy: 0.2869\n",
            "Epoch 52/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.2895 - accuracy: 0.4272 - val_loss: 3.1554 - val_accuracy: 0.2638\n",
            "Epoch 53/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.2787 - accuracy: 0.4284 - val_loss: 2.9704 - val_accuracy: 0.2866\n",
            "Epoch 54/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.2670 - accuracy: 0.4328 - val_loss: 3.0779 - val_accuracy: 0.2734\n",
            "Epoch 55/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.2492 - accuracy: 0.4358 - val_loss: 2.9891 - val_accuracy: 0.2840\n",
            "Epoch 56/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.2390 - accuracy: 0.4379 - val_loss: 3.0485 - val_accuracy: 0.2767\n",
            "Epoch 57/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.2292 - accuracy: 0.4394 - val_loss: 3.0336 - val_accuracy: 0.2724\n",
            "Epoch 58/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.2185 - accuracy: 0.4419 - val_loss: 3.0841 - val_accuracy: 0.2727\n",
            "Epoch 59/200\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 2.2047 - accuracy: 0.4452 - val_loss: 3.0379 - val_accuracy: 0.2801\n",
            "Epoch 60/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.1941 - accuracy: 0.4480 - val_loss: 3.0183 - val_accuracy: 0.2860\n",
            "Epoch 61/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.1795 - accuracy: 0.4490 - val_loss: 3.0400 - val_accuracy: 0.2782\n",
            "Epoch 62/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.1684 - accuracy: 0.4507 - val_loss: 2.9693 - val_accuracy: 0.2913\n",
            "Epoch 63/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.1566 - accuracy: 0.4543 - val_loss: 3.0591 - val_accuracy: 0.2775\n",
            "Epoch 64/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.1506 - accuracy: 0.4532 - val_loss: 3.0349 - val_accuracy: 0.2824\n",
            "Epoch 65/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.1398 - accuracy: 0.4576 - val_loss: 3.0785 - val_accuracy: 0.2703\n",
            "Epoch 66/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.1267 - accuracy: 0.4632 - val_loss: 2.9773 - val_accuracy: 0.2888\n",
            "Epoch 67/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.1152 - accuracy: 0.4636 - val_loss: 2.9723 - val_accuracy: 0.2941\n",
            "Epoch 68/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.1058 - accuracy: 0.4650 - val_loss: 3.0077 - val_accuracy: 0.2868\n",
            "Epoch 69/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.0978 - accuracy: 0.4661 - val_loss: 3.0086 - val_accuracy: 0.2838\n",
            "Epoch 70/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.0858 - accuracy: 0.4689 - val_loss: 2.9224 - val_accuracy: 0.2962\n",
            "Epoch 71/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.0756 - accuracy: 0.4745 - val_loss: 2.9450 - val_accuracy: 0.2960\n",
            "Epoch 72/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.0660 - accuracy: 0.4725 - val_loss: 3.0114 - val_accuracy: 0.2919\n",
            "Epoch 73/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.0598 - accuracy: 0.4737 - val_loss: 2.9541 - val_accuracy: 0.2962\n",
            "Epoch 74/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.0463 - accuracy: 0.4770 - val_loss: 3.0454 - val_accuracy: 0.2822\n",
            "Epoch 75/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.0398 - accuracy: 0.4794 - val_loss: 3.0033 - val_accuracy: 0.2929\n",
            "Epoch 76/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.0315 - accuracy: 0.4815 - val_loss: 2.9650 - val_accuracy: 0.2984\n",
            "Epoch 77/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.0205 - accuracy: 0.4829 - val_loss: 3.0639 - val_accuracy: 0.2890\n",
            "Epoch 78/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.0110 - accuracy: 0.4846 - val_loss: 3.0345 - val_accuracy: 0.2889\n",
            "Epoch 79/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 2.0029 - accuracy: 0.4874 - val_loss: 2.9981 - val_accuracy: 0.2966\n",
            "Epoch 80/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.9931 - accuracy: 0.4884 - val_loss: 3.0007 - val_accuracy: 0.2922\n",
            "Epoch 81/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.9854 - accuracy: 0.4907 - val_loss: 2.9434 - val_accuracy: 0.3031\n",
            "Epoch 82/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.9786 - accuracy: 0.4920 - val_loss: 2.9695 - val_accuracy: 0.2988\n",
            "Epoch 83/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.9693 - accuracy: 0.4928 - val_loss: 3.1230 - val_accuracy: 0.2800\n",
            "Epoch 84/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.9611 - accuracy: 0.4935 - val_loss: 2.9873 - val_accuracy: 0.2961\n",
            "Epoch 85/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.9524 - accuracy: 0.4979 - val_loss: 2.9847 - val_accuracy: 0.2965\n",
            "Epoch 86/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.9469 - accuracy: 0.4988 - val_loss: 3.0225 - val_accuracy: 0.2972\n",
            "Epoch 87/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.9394 - accuracy: 0.4991 - val_loss: 3.0578 - val_accuracy: 0.2884\n",
            "Epoch 88/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.9336 - accuracy: 0.5011 - val_loss: 2.9744 - val_accuracy: 0.2961\n",
            "Epoch 89/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.9258 - accuracy: 0.5013 - val_loss: 3.0552 - val_accuracy: 0.2876\n",
            "Epoch 90/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.9169 - accuracy: 0.5049 - val_loss: 2.9392 - val_accuracy: 0.3034\n",
            "Epoch 91/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.9087 - accuracy: 0.5070 - val_loss: 3.0566 - val_accuracy: 0.2927\n",
            "Epoch 92/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.9060 - accuracy: 0.5076 - val_loss: 2.9624 - val_accuracy: 0.3026\n",
            "Epoch 93/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.8984 - accuracy: 0.5069 - val_loss: 3.0744 - val_accuracy: 0.2850\n",
            "Epoch 94/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.8884 - accuracy: 0.5103 - val_loss: 2.9132 - val_accuracy: 0.3060\n",
            "Epoch 95/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.8845 - accuracy: 0.5120 - val_loss: 3.0011 - val_accuracy: 0.2994\n",
            "Epoch 96/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.8781 - accuracy: 0.5143 - val_loss: 2.9478 - val_accuracy: 0.3052\n",
            "Epoch 97/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.8714 - accuracy: 0.5140 - val_loss: 3.1242 - val_accuracy: 0.2804\n",
            "Epoch 98/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.8642 - accuracy: 0.5163 - val_loss: 2.9487 - val_accuracy: 0.3058\n",
            "Epoch 99/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.8595 - accuracy: 0.5174 - val_loss: 2.9670 - val_accuracy: 0.3038\n",
            "Epoch 100/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.8511 - accuracy: 0.5174 - val_loss: 3.0404 - val_accuracy: 0.2921\n",
            "Epoch 101/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.8474 - accuracy: 0.5175 - val_loss: 2.9878 - val_accuracy: 0.3046\n",
            "Epoch 102/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.8376 - accuracy: 0.5239 - val_loss: 3.0491 - val_accuracy: 0.2923\n",
            "Epoch 103/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.8328 - accuracy: 0.5220 - val_loss: 3.0238 - val_accuracy: 0.2980\n",
            "Epoch 104/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.8299 - accuracy: 0.5217 - val_loss: 2.9999 - val_accuracy: 0.3033\n",
            "Epoch 105/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.8217 - accuracy: 0.5264 - val_loss: 3.0532 - val_accuracy: 0.2945\n",
            "Epoch 106/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.8190 - accuracy: 0.5264 - val_loss: 3.0065 - val_accuracy: 0.3005\n",
            "Epoch 107/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.8092 - accuracy: 0.5281 - val_loss: 2.9471 - val_accuracy: 0.3109\n",
            "Epoch 108/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.8070 - accuracy: 0.5287 - val_loss: 3.0929 - val_accuracy: 0.2944\n",
            "Epoch 109/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.7993 - accuracy: 0.5289 - val_loss: 3.0699 - val_accuracy: 0.2941\n",
            "Epoch 110/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.7939 - accuracy: 0.5315 - val_loss: 2.9796 - val_accuracy: 0.3049\n",
            "Epoch 111/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.7894 - accuracy: 0.5319 - val_loss: 3.0567 - val_accuracy: 0.3013\n",
            "Epoch 112/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.7850 - accuracy: 0.5305 - val_loss: 3.0665 - val_accuracy: 0.2932\n",
            "Epoch 113/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.7791 - accuracy: 0.5343 - val_loss: 2.9871 - val_accuracy: 0.3063\n",
            "Epoch 114/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.7733 - accuracy: 0.5357 - val_loss: 3.0100 - val_accuracy: 0.3024\n",
            "Epoch 115/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.7698 - accuracy: 0.5367 - val_loss: 2.9618 - val_accuracy: 0.3087\n",
            "Epoch 116/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.7629 - accuracy: 0.5393 - val_loss: 3.0693 - val_accuracy: 0.2959\n",
            "Epoch 117/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.7610 - accuracy: 0.5387 - val_loss: 3.1018 - val_accuracy: 0.2927\n",
            "Epoch 118/200\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 1.7533 - accuracy: 0.5393 - val_loss: 3.0211 - val_accuracy: 0.3042\n",
            "Epoch 119/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.7504 - accuracy: 0.5379 - val_loss: 3.0496 - val_accuracy: 0.3025\n",
            "Epoch 120/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.7456 - accuracy: 0.5389 - val_loss: 3.0522 - val_accuracy: 0.2998\n",
            "Epoch 121/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.7396 - accuracy: 0.5427 - val_loss: 3.0961 - val_accuracy: 0.2962\n",
            "Epoch 122/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.7342 - accuracy: 0.5434 - val_loss: 2.9967 - val_accuracy: 0.3071\n",
            "Epoch 123/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.7295 - accuracy: 0.5466 - val_loss: 3.0861 - val_accuracy: 0.2982\n",
            "Epoch 124/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.7259 - accuracy: 0.5462 - val_loss: 3.1815 - val_accuracy: 0.2862\n",
            "Epoch 125/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.7234 - accuracy: 0.5447 - val_loss: 3.0100 - val_accuracy: 0.3077\n",
            "Epoch 126/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.7186 - accuracy: 0.5466 - val_loss: 3.0434 - val_accuracy: 0.3049\n",
            "Epoch 127/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.7106 - accuracy: 0.5488 - val_loss: 3.1098 - val_accuracy: 0.2945\n",
            "Epoch 128/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.7074 - accuracy: 0.5495 - val_loss: 3.0296 - val_accuracy: 0.3083\n",
            "Epoch 129/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.7045 - accuracy: 0.5500 - val_loss: 3.0652 - val_accuracy: 0.3043\n",
            "Epoch 130/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6987 - accuracy: 0.5498 - val_loss: 3.0350 - val_accuracy: 0.3060\n",
            "Epoch 131/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6990 - accuracy: 0.5518 - val_loss: 3.0829 - val_accuracy: 0.2991\n",
            "Epoch 132/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6936 - accuracy: 0.5518 - val_loss: 3.0458 - val_accuracy: 0.3034\n",
            "Epoch 133/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6877 - accuracy: 0.5517 - val_loss: 3.0754 - val_accuracy: 0.2997\n",
            "Epoch 134/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6854 - accuracy: 0.5542 - val_loss: 3.1010 - val_accuracy: 0.2990\n",
            "Epoch 135/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6763 - accuracy: 0.5568 - val_loss: 3.1270 - val_accuracy: 0.3008\n",
            "Epoch 136/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6752 - accuracy: 0.5571 - val_loss: 3.0538 - val_accuracy: 0.3051\n",
            "Epoch 137/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6691 - accuracy: 0.5581 - val_loss: 2.9945 - val_accuracy: 0.3077\n",
            "Epoch 138/200\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 1.6630 - accuracy: 0.5596 - val_loss: 3.0575 - val_accuracy: 0.3097\n",
            "Epoch 139/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6599 - accuracy: 0.5599 - val_loss: 3.0638 - val_accuracy: 0.3078\n",
            "Epoch 140/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6588 - accuracy: 0.5590 - val_loss: 3.0387 - val_accuracy: 0.3069\n",
            "Epoch 141/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6543 - accuracy: 0.5613 - val_loss: 3.0817 - val_accuracy: 0.3003\n",
            "Epoch 142/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6508 - accuracy: 0.5626 - val_loss: 3.0479 - val_accuracy: 0.3059\n",
            "Epoch 143/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6502 - accuracy: 0.5610 - val_loss: 3.0287 - val_accuracy: 0.3101\n",
            "Epoch 144/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6439 - accuracy: 0.5631 - val_loss: 3.0658 - val_accuracy: 0.3041\n",
            "Epoch 145/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6392 - accuracy: 0.5637 - val_loss: 3.1529 - val_accuracy: 0.2946\n",
            "Epoch 146/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6373 - accuracy: 0.5644 - val_loss: 2.9866 - val_accuracy: 0.3206\n",
            "Epoch 147/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6329 - accuracy: 0.5674 - val_loss: 3.1351 - val_accuracy: 0.2965\n",
            "Epoch 148/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6328 - accuracy: 0.5645 - val_loss: 3.1214 - val_accuracy: 0.3018\n",
            "Epoch 149/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6265 - accuracy: 0.5662 - val_loss: 3.0561 - val_accuracy: 0.3056\n",
            "Epoch 150/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6230 - accuracy: 0.5664 - val_loss: 3.1611 - val_accuracy: 0.2999\n",
            "Epoch 151/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6190 - accuracy: 0.5686 - val_loss: 3.0906 - val_accuracy: 0.3087\n",
            "Epoch 152/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6178 - accuracy: 0.5665 - val_loss: 3.1204 - val_accuracy: 0.3033\n",
            "Epoch 153/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6092 - accuracy: 0.5707 - val_loss: 3.1696 - val_accuracy: 0.2942\n",
            "Epoch 154/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6098 - accuracy: 0.5711 - val_loss: 3.0514 - val_accuracy: 0.3108\n",
            "Epoch 155/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6062 - accuracy: 0.5719 - val_loss: 3.1160 - val_accuracy: 0.3007\n",
            "Epoch 156/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.6029 - accuracy: 0.5735 - val_loss: 3.0737 - val_accuracy: 0.3096\n",
            "Epoch 157/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5961 - accuracy: 0.5732 - val_loss: 3.0974 - val_accuracy: 0.3059\n",
            "Epoch 158/200\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 1.5952 - accuracy: 0.5746 - val_loss: 3.1095 - val_accuracy: 0.3031\n",
            "Epoch 159/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5912 - accuracy: 0.5732 - val_loss: 3.2791 - val_accuracy: 0.2897\n",
            "Epoch 160/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5860 - accuracy: 0.5764 - val_loss: 3.1287 - val_accuracy: 0.3045\n",
            "Epoch 161/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5865 - accuracy: 0.5748 - val_loss: 3.1831 - val_accuracy: 0.2983\n",
            "Epoch 162/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5826 - accuracy: 0.5772 - val_loss: 3.2737 - val_accuracy: 0.2897\n",
            "Epoch 163/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5773 - accuracy: 0.5785 - val_loss: 3.1740 - val_accuracy: 0.3004\n",
            "Epoch 164/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5775 - accuracy: 0.5794 - val_loss: 3.1129 - val_accuracy: 0.3057\n",
            "Epoch 165/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5757 - accuracy: 0.5770 - val_loss: 3.1215 - val_accuracy: 0.3102\n",
            "Epoch 166/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5721 - accuracy: 0.5770 - val_loss: 3.1586 - val_accuracy: 0.3006\n",
            "Epoch 167/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5687 - accuracy: 0.5784 - val_loss: 3.1136 - val_accuracy: 0.3065\n",
            "Epoch 168/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5655 - accuracy: 0.5815 - val_loss: 3.1242 - val_accuracy: 0.3040\n",
            "Epoch 169/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5615 - accuracy: 0.5818 - val_loss: 3.1823 - val_accuracy: 0.3043\n",
            "Epoch 170/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5573 - accuracy: 0.5799 - val_loss: 3.1703 - val_accuracy: 0.3007\n",
            "Epoch 171/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5576 - accuracy: 0.5825 - val_loss: 3.1242 - val_accuracy: 0.3065\n",
            "Epoch 172/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5518 - accuracy: 0.5827 - val_loss: 3.1071 - val_accuracy: 0.3092\n",
            "Epoch 173/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5492 - accuracy: 0.5816 - val_loss: 3.1668 - val_accuracy: 0.3051\n",
            "Epoch 174/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5472 - accuracy: 0.5830 - val_loss: 3.1757 - val_accuracy: 0.3003\n",
            "Epoch 175/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5483 - accuracy: 0.5819 - val_loss: 3.1271 - val_accuracy: 0.3059\n",
            "Epoch 176/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5422 - accuracy: 0.5853 - val_loss: 3.1842 - val_accuracy: 0.2974\n",
            "Epoch 177/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5370 - accuracy: 0.5859 - val_loss: 3.1504 - val_accuracy: 0.3036\n",
            "Epoch 178/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5392 - accuracy: 0.5851 - val_loss: 3.2109 - val_accuracy: 0.2983\n",
            "Epoch 179/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5383 - accuracy: 0.5843 - val_loss: 3.1219 - val_accuracy: 0.3060\n",
            "Epoch 180/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5363 - accuracy: 0.5864 - val_loss: 3.2385 - val_accuracy: 0.2956\n",
            "Epoch 181/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5308 - accuracy: 0.5882 - val_loss: 3.2149 - val_accuracy: 0.2979\n",
            "Epoch 182/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5285 - accuracy: 0.5883 - val_loss: 3.1375 - val_accuracy: 0.3073\n",
            "Epoch 183/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5220 - accuracy: 0.5894 - val_loss: 3.1694 - val_accuracy: 0.3026\n",
            "Epoch 184/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5262 - accuracy: 0.5867 - val_loss: 3.1664 - val_accuracy: 0.3044\n",
            "Epoch 185/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5197 - accuracy: 0.5904 - val_loss: 3.1958 - val_accuracy: 0.3018\n",
            "Epoch 186/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5192 - accuracy: 0.5899 - val_loss: 3.1811 - val_accuracy: 0.3037\n",
            "Epoch 187/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5113 - accuracy: 0.5919 - val_loss: 3.1387 - val_accuracy: 0.3113\n",
            "Epoch 188/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5127 - accuracy: 0.5898 - val_loss: 3.2072 - val_accuracy: 0.3019\n",
            "Epoch 189/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5078 - accuracy: 0.5911 - val_loss: 3.1754 - val_accuracy: 0.3051\n",
            "Epoch 190/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5101 - accuracy: 0.5901 - val_loss: 3.3456 - val_accuracy: 0.2855\n",
            "Epoch 191/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.5054 - accuracy: 0.5903 - val_loss: 3.2348 - val_accuracy: 0.2983\n",
            "Epoch 192/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.4990 - accuracy: 0.5948 - val_loss: 3.2184 - val_accuracy: 0.2985\n",
            "Epoch 193/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.4994 - accuracy: 0.5933 - val_loss: 3.1577 - val_accuracy: 0.3076\n",
            "Epoch 194/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.4943 - accuracy: 0.5949 - val_loss: 3.2530 - val_accuracy: 0.2974\n",
            "Epoch 195/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.4934 - accuracy: 0.5959 - val_loss: 3.1808 - val_accuracy: 0.3061\n",
            "Epoch 196/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.4938 - accuracy: 0.5955 - val_loss: 3.2654 - val_accuracy: 0.2937\n",
            "Epoch 197/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.4887 - accuracy: 0.5969 - val_loss: 3.1847 - val_accuracy: 0.3053\n",
            "Epoch 198/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.4856 - accuracy: 0.5962 - val_loss: 3.2180 - val_accuracy: 0.3025\n",
            "Epoch 199/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.4872 - accuracy: 0.5941 - val_loss: 3.1779 - val_accuracy: 0.3045\n",
            "Epoch 200/200\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 1.4829 - accuracy: 0.5981 - val_loss: 3.3436 - val_accuracy: 0.2902\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_QICA0vECJf"
      },
      "source": [
        "model.save_weights(filepath=F\"/content/gdrive/My Drive/Checkpoints/InceptionV2_SGD_Dropout.h5\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK9Ww8AJYaaM"
      },
      "source": [
        "## Test Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZuWXGeW-nqY",
        "outputId": "3b6e481c-e356-466d-dd58-2a327401e683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "print(history_dict.keys())\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(history_dict['accuracy']) + 1)\n",
        "\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfO0lHJIE0Gy",
        "outputId": "11437c27-e443-49ee-b9b2-f4711c0ae0eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "y_pred = model.predict(x_test).argmax(-1)\n",
        "import sklearn.metrics as metrics\n",
        "print(metrics.classification_report(y_test.argmax(axis=1), y_pred))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.09      0.15       100\n",
            "           1       0.40      0.40      0.40       100\n",
            "           2       0.17      0.07      0.10       100\n",
            "           3       0.17      0.12      0.14       100\n",
            "           4       0.11      0.24      0.15       100\n",
            "           5       0.35      0.17      0.23       100\n",
            "           6       0.29      0.15      0.20       100\n",
            "           7       0.34      0.35      0.34       100\n",
            "           8       0.68      0.43      0.53       100\n",
            "           9       0.67      0.20      0.31       100\n",
            "          10       0.21      0.23      0.22       100\n",
            "          11       0.46      0.11      0.18       100\n",
            "          12       0.51      0.19      0.28       100\n",
            "          13       0.37      0.17      0.23       100\n",
            "          14       0.32      0.24      0.28       100\n",
            "          15       0.32      0.24      0.28       100\n",
            "          16       0.62      0.24      0.35       100\n",
            "          17       0.45      0.43      0.44       100\n",
            "          18       0.18      0.06      0.09       100\n",
            "          19       0.26      0.11      0.15       100\n",
            "          20       0.82      0.50      0.62       100\n",
            "          21       0.26      0.50      0.34       100\n",
            "          22       0.45      0.29      0.35       100\n",
            "          23       0.21      0.76      0.33       100\n",
            "          24       0.34      0.55      0.42       100\n",
            "          25       0.33      0.18      0.23       100\n",
            "          26       0.21      0.35      0.26       100\n",
            "          27       0.18      0.40      0.25       100\n",
            "          28       0.47      0.57      0.52       100\n",
            "          29       0.28      0.12      0.17       100\n",
            "          30       0.39      0.30      0.34       100\n",
            "          31       0.31      0.34      0.33       100\n",
            "          32       0.30      0.16      0.21       100\n",
            "          33       0.25      0.27      0.26       100\n",
            "          34       0.27      0.27      0.27       100\n",
            "          35       0.28      0.08      0.12       100\n",
            "          36       0.25      0.36      0.30       100\n",
            "          37       0.37      0.19      0.25       100\n",
            "          38       0.17      0.18      0.18       100\n",
            "          39       0.24      0.08      0.12       100\n",
            "          40       0.41      0.27      0.33       100\n",
            "          41       0.69      0.54      0.61       100\n",
            "          42       0.23      0.49      0.32       100\n",
            "          43       0.18      0.40      0.24       100\n",
            "          44       0.12      0.11      0.11       100\n",
            "          45       0.11      0.10      0.11       100\n",
            "          46       0.27      0.20      0.23       100\n",
            "          47       0.35      0.53      0.42       100\n",
            "          48       0.66      0.55      0.60       100\n",
            "          49       0.23      0.73      0.36       100\n",
            "          50       0.10      0.13      0.12       100\n",
            "          51       0.33      0.22      0.27       100\n",
            "          52       0.49      0.65      0.56       100\n",
            "          53       0.40      0.37      0.38       100\n",
            "          54       0.45      0.23      0.30       100\n",
            "          55       0.12      0.05      0.07       100\n",
            "          56       0.61      0.33      0.43       100\n",
            "          57       0.47      0.16      0.24       100\n",
            "          58       0.56      0.33      0.42       100\n",
            "          59       0.26      0.35      0.30       100\n",
            "          60       0.72      0.58      0.64       100\n",
            "          61       0.57      0.46      0.51       100\n",
            "          62       0.31      0.16      0.21       100\n",
            "          63       0.15      0.53      0.23       100\n",
            "          64       0.08      0.11      0.09       100\n",
            "          65       0.10      0.07      0.08       100\n",
            "          66       0.14      0.31      0.19       100\n",
            "          67       0.20      0.40      0.27       100\n",
            "          68       0.61      0.66      0.63       100\n",
            "          69       0.64      0.27      0.38       100\n",
            "          70       0.35      0.22      0.27       100\n",
            "          71       0.55      0.06      0.11       100\n",
            "          72       0.15      0.09      0.11       100\n",
            "          73       0.33      0.22      0.27       100\n",
            "          74       0.10      0.32      0.16       100\n",
            "          75       0.33      0.65      0.44       100\n",
            "          76       0.33      0.68      0.44       100\n",
            "          77       0.13      0.19      0.15       100\n",
            "          78       0.07      0.10      0.08       100\n",
            "          79       0.35      0.37      0.36       100\n",
            "          80       0.09      0.07      0.08       100\n",
            "          81       0.42      0.16      0.23       100\n",
            "          82       0.67      0.46      0.54       100\n",
            "          83       0.09      0.01      0.02       100\n",
            "          84       0.16      0.05      0.08       100\n",
            "          85       0.40      0.48      0.43       100\n",
            "          86       0.30      0.30      0.30       100\n",
            "          87       0.74      0.32      0.45       100\n",
            "          88       0.20      0.17      0.18       100\n",
            "          89       0.38      0.26      0.31       100\n",
            "          90       0.28      0.23      0.25       100\n",
            "          91       0.39      0.44      0.41       100\n",
            "          92       0.42      0.16      0.23       100\n",
            "          93       0.14      0.25      0.18       100\n",
            "          94       0.72      0.60      0.66       100\n",
            "          95       0.50      0.37      0.43       100\n",
            "          96       0.32      0.32      0.32       100\n",
            "          97       0.24      0.28      0.26       100\n",
            "          98       0.14      0.06      0.08       100\n",
            "          99       0.13      0.15      0.14       100\n",
            "\n",
            "    accuracy                           0.29     10000\n",
            "   macro avg       0.34      0.29      0.29     10000\n",
            "weighted avg       0.34      0.29      0.29     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8SADQsAT1JV"
      },
      "source": [
        "## TESTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4Smel9NqS1r"
      },
      "source": [
        "model.load_weights(filepath=F\"/content/gdrive/My Drive/Checkpoints/InceptionV2_SGD_Dropout.h5\")\n",
        "\n",
        "# model.load_weights(\"InceptionV2_SGD_Dropout.h5\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0XDBMyDq0d3",
        "outputId": "12a689e6-b20d-40a5-b779-5267678871dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "result = model.evaluate(batch_size=128, x=x_test, y=y_test)\n",
        "dict(zip(model.metrics_names,result))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79/79 [==============================] - 1s 12ms/step - loss: 3.3436 - accuracy: 0.2902\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.29019999504089355, 'loss': 3.343613386154175}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgZYqnfyWKOM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}